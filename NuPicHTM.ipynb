{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Run swarm '"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# OPF Swarming to Optimize Algorithm for Predictions (and the simulated data)\n",
    "\n",
    "\"\"\"  Command line password edit for MySQL \"\"\"\n",
    "# export NTA_CONF_PROP_nupic_cluster_database_passwd=MySQL123\n",
    "\n",
    "\"\"\" Run swarm \"\"\"\n",
    "# ~/nupic/scripts/run_swarm.py --overwrite ~/myswarm/data/search_def.json \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set up opf model and extract anomalies from data\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "import yaml\n",
    "import csv\n",
    "import datetime\n",
    "from nupic.algorithms import anomaly_likelihood\n",
    "from nupic.frameworks.opf.model_factory import ModelFactory\n",
    "import importlib\n",
    "import SimAnomalyDataset as sim\n",
    "\n",
    "modelParams = importlib.import_module(\"model_params\").MODEL_PARAMS # best_model_params / model_params\n",
    "\n",
    "# Create dataset\n",
    "datalabels=[\"dttm\",\"value\"]\n",
    "data, anomaly_loc, anomaly_dur, dates = sim.get_data(n=0,datalabels=datalabels)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +=: 'float' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-20e0718dcef2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         anomaly_Likelihood_r = anomaly_likelihood_helper.anomalyProbability(\n\u001b[0;32m---> 52\u001b[0;31m             \u001b[0mmodelInput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdatalabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manomaly_score_r\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodelInput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdatalabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         )\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/nupic/algorithms/anomaly_likelihood.pyc\u001b[0m in \u001b[0;36manomalyProbability\u001b[0;34m(self, value, anomalyScore, timestamp)\u001b[0m\n\u001b[1;32m    384\u001b[0m         _, _, self._distribution = estimateAnomalyLikelihoods(\n\u001b[1;32m    385\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_historicalScores\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 386\u001b[0;31m           skipRecords=numSkipRecords)\n\u001b[0m\u001b[1;32m    387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m       likelihoods, _, self._distribution = updateAnomalyLikelihoods(\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/nupic/algorithms/anomaly_likelihood.pyc\u001b[0m in \u001b[0;36mestimateAnomalyLikelihoods\u001b[0;34m(anomalyScores, averagingWindow, skipRecords, verbosity)\u001b[0m\n\u001b[1;32m    458\u001b[0m     \u001b[0manomalyScores\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0mwindowSize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maveragingWindow\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m     verbosity = verbosity)\n\u001b[0m\u001b[1;32m    461\u001b[0m   \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maggRecordList\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m   \u001b[0mdataValues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/nupic/algorithms/anomaly_likelihood.pyc\u001b[0m in \u001b[0;36m_anomalyScoreMovingAverage\u001b[0;34m(anomalyScores, windowSize, verbosity)\u001b[0m\n\u001b[1;32m    670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m     avg, historicalValues, total = (\n\u001b[0;32m--> 672\u001b[0;31m       \u001b[0mMovingAverage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistoricalValues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindowSize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    673\u001b[0m       )\n\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/nupic/utils.pyc\u001b[0m in \u001b[0;36mcompute\u001b[0;34m(slidingWindow, total, newVal, windowSize)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0mslidingWindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnewVal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m     \u001b[0mtotal\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnewVal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslidingWindow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslidingWindow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +=: 'float' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create OPF Model & Load parameters into model space\n",
    "model = ModelFactory.create(modelParams)\n",
    "\n",
    "# What to predict?\n",
    "model.enableInference({'predictedField': datalabels[1]})\n",
    "\n",
    "# Open the file to loop over each row to feed model\n",
    "output = []\n",
    "anomaly_score = []\n",
    "prediction = []\n",
    "confidence = []\n",
    "anomaly_Likelihood = []\n",
    "anomaly_logLikelihood = []\n",
    "anomaly_likelihood_helper = anomaly_likelihood.AnomalyLikelihood()\n",
    "input_data = []\n",
    "with open (\"sim_data.csv\") as fileIn:\n",
    "    reader = csv.reader(fileIn)\n",
    "    # The first two rows are not data, but we'll need the field names when passing data into the model.\n",
    "    headers = reader.next()\n",
    "    reader.next()\n",
    "    reader.next()\n",
    "\n",
    "    # loop through data rows\n",
    "    for record in reader:\n",
    "        # Save input data for plotting\n",
    "        input_data.append(record)\n",
    "        \n",
    "        # Create a dictionary with field names as keys, row values as values.\n",
    "        modelInput = dict(zip(headers, record))\n",
    "        \n",
    "        # Convert string consumption to float value.\n",
    "        modelInput[datalabels[1]] = float(modelInput[datalabels[1]])\n",
    "        \n",
    "        # Convert timestamp string to Python datetime.\n",
    "        modelInput[datalabels[0]] = datetime.datetime.strptime(\n",
    "          modelInput[datalabels[0]], \"%Y-%m-%d %H:%M:%S\")\n",
    "        \n",
    "        # Push the data into the model and get back results.\n",
    "        result = model.run(modelInput)\n",
    "        \n",
    "        # Save predicition history in new file\n",
    "        output.append(result.inferences['multiStepBestPredictions'][1])\n",
    "        \n",
    "        anomaly_score_r = result.inferences[\"anomalyScore\"]\n",
    "        \n",
    "        prediction_r = result.inferences[\"multiStepBestPredictions\"][1]\n",
    "        \n",
    "        confidence.append(result.inferences[\"multiStepPredictions\"][1][prediction_r])\n",
    "\n",
    "        anomaly_Likelihood_r = anomaly_likelihood_helper.anomalyProbability(\n",
    "            modelInput[datalabels[1]], anomaly_score_r, modelInput[datalabels[0]]\n",
    "        )\n",
    "        \n",
    "        anomaly_logLikelihood_r = anomaly_likelihood_helper.computeLogLikelihood(anomaly_Likelihood_r)\n",
    "        \n",
    "        #store for later\n",
    "        anomaly_score.append(anomaly_score_r)\n",
    "        prediction.append(prediction_r)\n",
    "        anomaly_Likelihood.append(anomaly_Likelihood_r)\n",
    "        anomaly_logLikelihood.append(anomaly_logLikelihood_r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluatePredictions as evalPred\n",
    "\n",
    "# slight data transformation\n",
    "a = np.asarray(input_data)\n",
    "a = a[:,2].astype(np.float)\n",
    "\n",
    "metric = \"RMSE\"\n",
    "OUT = evalPred.main(test=a[8000:], pred=prediction[8000:], metric = \"MSE\")\n",
    "\n",
    "#RMSE plot\n",
    "thresh_min = 200\n",
    "thresh_max = 500\n",
    "to_plot=np.asarray(OUT)\n",
    "to_plot[to_plot<thresh_min]=0\n",
    "to_plot[to_plot>thresh_max]=thresh_max\n",
    "to_plot = (to_plot/np.max(to_plot)) #remove normalizer whilst thresholding\n",
    "\n",
    "sim.plot_data((to_plot), anomaly_loc, anomaly_dur,title=metric)\n",
    "\n",
    "#anomaly likelihoods\n",
    "sim.plot_data((anomaly_score), anomaly_loc, anomaly_dur,title=\"anomaly_score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred=a[8000:]-prediction[8000:]\n",
    "evalPred.GaussianPredError(pred,anomaly_loc, anomaly_dur,thresh=0.97)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "635px",
    "left": "1437.99px",
    "right": "20px",
    "top": "152.988px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
