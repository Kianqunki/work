{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, ast\n",
    "from elasticsearch import Elasticsearch\n",
    "import pprint\n",
    "import datetime\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set elasticsearch URL host\n",
    "es = Elasticsearch(hosts = [{\"host\" : \"192.168.188.38\", \"port\" : 9200}])\n",
    "\n",
    "# Set index within elasticsearch host\n",
    "es_index = \"wixed-production_8707cb7b-738e-4507-9899-8bac0f49c8fc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def queryData(index, timestamp=None, ival=\"day\", timestampasstring=False) :\n",
    "    '''\n",
    "    Function for querying URL index using elasticsearch \n",
    "\n",
    "\n",
    "    Variables\n",
    "    ---------\n",
    "    index             : Reference to host sub group\n",
    "    timestamp         : Specify index time stamp\n",
    "    ival              : Specify aggregation interval\n",
    "    timestampasstring : String version of time stamp\n",
    "\n",
    "    Returns\n",
    "    ---------\n",
    "    res               : Aggregation results\n",
    "    '''\n",
    "\n",
    "    #index = es_index \n",
    "    #timestamp=None\n",
    "    #ival=\"day\"\n",
    "    #timestampasstring=False\n",
    "    \n",
    "    DATETIME_FORMAT = \"%Y-%m-%dT%H:%M:%S.%fZ\"\n",
    "\n",
    "    # If inputted, edit timestamp with specified format\n",
    "    if timestamp is not None: \n",
    "\n",
    "        timestamp = timestamp.strftime(DATETIME_FORMAT)\n",
    "\n",
    "    # Extract sorted timestamp from host index\n",
    "    else: \n",
    "\n",
    "        res_timestamp = es.search(\n",
    "            index = index,\n",
    "            body={\n",
    "                \"query\":{\n",
    "                    \"bool\": {\n",
    "                        \"must\": {\n",
    "                            \"match_all\": {}\n",
    "                        },\n",
    "                    }\n",
    "                },\n",
    "                # Sort documents by timestamp, ignoring fields with no timestamp\n",
    "                \"sort\": {\"@timestamp\": {\"unmapped_type\" : \"long\"}},\n",
    "                \"size\": 1,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Error handling if no results are returned\n",
    "        if res_timestamp[\"hits\"][\"hits\"]:\n",
    "            timestamp = res_timestamp[\"hits\"][\"hits\"][0][\"_source\"][\"@timestamp\"]\n",
    "\n",
    "    #print(timestamp, type(timestamp))\n",
    "    # timestamp = \"2017-04-01T00:00:00.000Z\"\n",
    "    #timestamp = \"13/Jun/2017:00:00:00 +0000\"\n",
    "\n",
    "    # if timestamp is None:\n",
    "    #     bool_query = {\"bool\": {\n",
    "    #                       \"must\": {\n",
    "    #                         \"match_all\": {}\n",
    "    #                       },\n",
    "    #                     }request_type_get       471.0\n",
    "    #                     }\n",
    "    # else:\n",
    "\n",
    "    # Creater query to extract greater than extracted timestamp *most recent\n",
    "    bool_query = {\"bool\": {\n",
    "                    \"must\": {\n",
    "                        \"match_all\": {}\n",
    "                            },\n",
    "                            \"filter\": {\n",
    "                                \"range\": {\n",
    "                                    \"@timestamp\" : {\n",
    "                                        \"gt\" : timestamp,\n",
    "                                                    }\n",
    "                                        }\n",
    "                    },\n",
    "                }\n",
    "            }\n",
    "\n",
    "    # Aggregation query groups timestamps into histogram\n",
    "    time_ival = {\n",
    "        \"date_histogram\" : {\n",
    "            \"field\" : \"@timestamp\",\n",
    "            \"interval\" : ival,\n",
    "            \"min_doc_count\" : 0, # to get buckets even when empty\n",
    "            \"extended_bounds\": { # get buckets for the complete time span requested\n",
    "                \"min\": timestamp,\n",
    "                \"max\": datetime.datetime.utcnow().strftime(DATETIME_FORMAT)\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Use ES to aggregate most recent timestamps within time intervals across requests\n",
    "    res = es.search(\n",
    "        index = index,\n",
    "        body={\n",
    "            \"query\": bool_query,\n",
    "            \"sort\": {\"@timestamp\": {\"unmapped_type\" : \"long\"}},\n",
    "            \"size\": 0,\n",
    "            \"aggs\":  {\n",
    "                \"by_request_type\": { # Top level aggregation: Group by request type\n",
    "                    \"terms\": {\n",
    "                        \"field\": \"verb.keyword\",\n",
    "                    },\n",
    "                    \"aggs\": { # Sub-aggregations : Group by time interval\n",
    "                        \"requests_per_ival\": time_ival\n",
    "                    }\n",
    "                },\n",
    "                \"requests_per_ival\": time_ival, # number of requests in histogram interval\n",
    "                # \"error_request_log\": { # Top level aggregation\n",
    "                #     \"terms\": {\n",
    "                #         \"field\": \"error_log\":\n",
    "                #     },\n",
    "                \"error_request_log\": {\n",
    "                    \"filter\": {\n",
    "                        \"match\": {\n",
    "                            \"error_log\": \"true\"\n",
    "                        }\n",
    "                    },\n",
    "                    \"aggs\": { # Sub-aggregations of error log\n",
    "                        \"requests_per_ival\": time_ival\n",
    "                    }\n",
    "                },\n",
    "\n",
    "            },\n",
    "            # \"query\": {\n",
    "            #     \"wildcard\": {\n",
    "            #         \"request\": \"*error*\"\n",
    "            #     }\n",
    "            # }\n",
    "        }\n",
    "        )\n",
    "\n",
    "        \n",
    "    return res\n",
    "\n",
    "#res_sub = {\"total\": res['hits']['total'], \"aggregations\": res[\"aggregations\"]}\n",
    "#print(res_sub)\n",
    "\n",
    "#pp = pprint.PrettyPrinter(indent=2)\n",
    "#pp.pprint(res)\n",
    "\n",
    "#print((\"%d documents found\" % res['hits']['total'])#\"ignore_unmapped\" : true)\n",
    "#for doc in res['hits']['hits']:\n",
    "#    print(\"%s) %s\" % (doc['_id'], doc['_source']))\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getData( index=es_index, timestamp=None, ival=\"day\", timestampasstring=False):\n",
    "    '''\n",
    "    Using various parsing functions to manipulate the data in different ways \n",
    "    \n",
    "    Parsing functions\n",
    "    ------------\n",
    "    parseData  :\n",
    "    parseData2 :\n",
    "    parseData3 :\n",
    "    '''\n",
    "    return parseData3(queryData(index, timestamp, ival, timestampasstring), timestampasstring)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parseData (res):\n",
    "\n",
    "    total_requests = res[\"aggregations\"][\"requests_per_ival\"][\"buckets\"]\n",
    "    request_types = res[\"aggregations\"][\"by_request_type\"][\"buckets\"]\n",
    "    error_logs = res[\"aggregations\"][\"error_request_log\"][\"requests_per_ival\"][\"buckets\"]\n",
    "\n",
    "    request_counts = {\"requests\": [], \"error_log\": [], \"timestamp\": []}\n",
    "    #request_counts[\"timestamp\"] = []\n",
    "\n",
    "\n",
    "    for request in total_requests:\n",
    "        request_counts[\"requests\"].append(request[\"doc_count\"])\n",
    "        request_counts[\"timestamp\"].append(request[\"key_as_string\"])\n",
    "\n",
    "    for bucket in error_logs:\n",
    "        request_counts[\"error_log\"].append(bucket[\"doc_count\"])\n",
    "\n",
    "    #print request_counts\n",
    "\n",
    "\n",
    "    for request in request_types:\n",
    "        request_type = request[\"key\"].lower()\n",
    "        request_counts[request_type]= []\n",
    "\n",
    "        for bucket in request[\"requests_per_ival\"][\"buckets\"]:\n",
    "            request_counts[request_type].append(bucket[\"doc_count\"])\n",
    "\n",
    "    return request_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parseData2 (res):\n",
    "\n",
    "\n",
    "    request_counts = []\n",
    "\n",
    "    for request in total_requests:\n",
    "        #request_counts.append({\"requests\":[, request[\"doc_count\"]})\n",
    "        request_counts.append({\"requests\":[request[\"key_as_string\"], request[\"doc_count\"]]})\n",
    "\n",
    "    print \"no. of all requests buckets: {}\".format(len(total_requests))\n",
    "    #\n",
    "    #\n",
    "    for request in request_types:\n",
    "        request_type = request[\"key\"].lower()\n",
    "        #request_counts[request_type]= []\n",
    "        print \"no. of {} requests buckets: {}\".format(request_type, len(request[\"requests_per_ival\"][\"buckets\"]))\n",
    "\n",
    "        for ix, bucket in enumerate(request[\"requests_per_ival\"][\"buckets\"]):\n",
    "            request_counts[ix][request_type] = [bucket[\"key_as_string\"],bucket[\"doc_count\"]]\n",
    "\n",
    "    #pp = pprint.PrettyPrinter(indent=2)\n",
    "    #pp.pprint(request_counts)\n",
    "\n",
    "    return request_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parseData3 (res, timestampAsString=False):\n",
    "\n",
    "    if res[\"hits\"][\"total\"] == 0:\n",
    "        return {}\n",
    "\n",
    "    total_requests = res[\"aggregations\"][\"requests_per_ival\"][\"buckets\"]\n",
    "    request_types = res[\"aggregations\"][\"by_request_type\"][\"buckets\"]\n",
    "    error_logs = res[\"aggregations\"][\"error_request_log\"][\"requests_per_ival\"][\"buckets\"]\n",
    "\n",
    "    request_counts = []\n",
    "\n",
    "    for request in total_requests:\n",
    "        if timestampAsString:\n",
    "            request_counts.append({\n",
    "                \"timestamp\": request[\"key_as_string\"],\n",
    "                \"requests\":request[\"doc_count\"]})\n",
    "        else:\n",
    "            request_counts.append({\n",
    "                \"timestamp\": datetime.datetime.strptime(request[\"key_as_string\"], DATETIME_FORMAT),\n",
    "                \"requests\":request[\"doc_count\"]})\n",
    "\n",
    "    for ix, bucket in enumerate(error_logs):\n",
    "        request_counts[ix][\"error_log\"] = bucket[\"doc_count\"]\n",
    "\n",
    "\n",
    "    print \"no. of all requests buckets: {}\".format(len(total_requests))\n",
    "    #\n",
    "    #\n",
    "    for request in request_types:\n",
    "        request_type = request[\"key\"].lower()\n",
    "        #request_counts[request_type]= []\n",
    "        print \"no. of {} requests buckets: {}\".format(request_type, len(request[\"requests_per_ival\"][\"buckets\"]))\n",
    "\n",
    "        for ix, bucket in enumerate(request[\"requests_per_ival\"][\"buckets\"]):\n",
    "            request_counts[ix][request_type] = bucket[\"doc_count\"]\n",
    "\n",
    "    # pp = pprint.PrettyPrinter(indent=2)\n",
    "    # pp.pprint(request_counts)\n",
    "\n",
    "    return request_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getPercentiles (index=es_index, timestamp=None, ival=\"day\", timestampasstring=False):\n",
    "    \"\"\"\n",
    "    Though elasticsearch offers to get percentiles, this doesn't work on\n",
    "    doc_count of aggregated data, see https://github.com/elastic/elasticsearch/issues/7703\n",
    "    \"\"\"\n",
    "\n",
    "    res = parseData(queryData(index, timestamp, ival, timestampasstring))\n",
    "    request_perc = {}\n",
    "\n",
    "    for key in res:\n",
    "        if key != \"timestamp\" and res[key]:\n",
    "            request_perc[key] = np.percentile(np.array(res[key]),99)\n",
    "\n",
    "    # total_requests = res[\"aggregations\"][\"requests_per_ival\"][\"buckets\"]\n",
    "    # request_perc = []\n",
    "    #\n",
    "    # for bucket in total_requests:\n",
    "    #     request_perc.append(bucket[\"doc_count\"])\n",
    "    # request_perc = np.array(request_perc)\n",
    "\n",
    "    #return np.percentile(request_perc,99)\n",
    "    return request_perc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
